So, by having these commutative universes, you try to do the following. You
try to have each higher-up type universe to contain the more lower-down type
universes and objects, okay? But this is strict, meaning a type universe cannot
contain itself inside of it. You cannot have a type universe as an object
inside of itself. So you avoid this more fundamental version of Ross's paradox
called Gibralt's paradox. Okay, so this is why this matters. For now, let's
leave aside all this type universe business and look at some more fundamental
properties of Lean. So Lean is a functional programming language, and everybody
who's ever done any functional programming or anybody who basically takes the
name of this field at face value probably knows that it's all about functions,
okay? So let's be asked, how do we define functions in Lean? We start here by
just looking at lambda expressions, which we can write with this fun keyword
or just a lambda symbol. This originates from the idea that this is lambda
calculus. The act of introducing a new function is called lambda abstraction,
okay? Lambda abstraction is a very powerful tool. And once you have a function,
well, you can give it some argument and see what it returns, okay? And for
instance, a function application is in a sense the elimination of functions. So
the introduction is by lambda abstraction, and the destruction or elimination
function is called lambda application, okay, or function application. And the
syntax for this is really, it's extremely concise. And if you're not used to
this, you might be a little bit surprised. So what you do is just you write the
name of the function, you put a space, and then you follow with the arguments.
The number of arguments is clear, so you can just do spaces, and if the spaces
are done, then it will know the thing that comes after it will no longer count
as an argument, okay? So once this is the case, we can now also realize that
passing multiple arguments to the lambda function could be done using Cartesian
products and tuples, but this is not a very functional approach. In functional
programming, instead, we pass multiple arguments by just having functions return
other functions. We can instead have, instead of A Cartesian product B arrow C,
we can have A arrow B arrow C, meaning if once we give the first argument, we
get a new function, which only depends on the second argument, and then gives
us the result, okay? Once this is accepted, we can move on, and now we can
use some, we can do some interesting stuff. For instance, and in a functional
programming language, functions are also first-class citizens, meaning they will
be treated as any other object. So functions, you can easily pass to a function
as if it were any other variables. In this way, we can define, for instance, a
function, which takes these two kinds of functions with particular types on both
sides of the arrow, and just defined as doing the composition of the functions,
and also using some first impute argument. This is rather natural, and depending
on how much experience you have with functional programming languages, it might
be kind of surprising that you can write. And this is really the normal thing
you could ever have in functional programming. And now we can do some even
more general stuff. Really, function composition, it doesn't really matter what
concrete types we have here, they just need to match up in the sense that the
first function that applies, its return type must match the input type of the
second function that is applied. And we can write this by introducing some more
variables that hold the types from the two endpoints of the arrows. And then,
well, we got this very general compositional composition function. So this is
so what you can do, and yeah, I think this is a rather basic, but hopefully
more or less sufficient introduction. Two other types which are interesting,
we already looked at the product type. Product type, yeah, makes sense. We
somehow store tuples and always have both, well, things it contains available.
There is a related concept that does it a little bit different, which is the
sum type. It instead stores either of them, okay? So it's either A or either
B and only stores one and knows which one it is. It's kind of similar to C++
Union, where you know in which variant you are. It's the same thing as a Rust
enum, if any of you know any Rust. And this is then, this is a rather easy
construct which will come up again later. Okay, so much for some basic tile
theory, some basic lambda calculus. Now let's get to the topic at hand, which is
how do we fucking prove anything with this now? How do we do any math, theorem
proving, where is this stuff? Okay, now how would we approach this? A first idea
could be to create a new type, the type called prop. And prop has objects, and
these objects, well, they are propositions, okay? So logical statements in the
language of mathematics. So we look at some propositional logic. Propositional
logic usually starts with some basic propositions, let's call them P and Q.
And now we have some type constructors which use P and Q to construct P and Q,
P or Q, not P, or P implies Q, all these things. And now we kind of have a way
of somehow mixing and matching those to pretty much create arbitrary statements
in propositional logic. Now further, after now just expressing statements, not
really sufficient, we also want to prove them. So we introduce another type
called, for each proposition P, we now introduce a new type, which depends on
P, called proof P. I should have explained dependent type theory earlier, what
the dependent really means. But anyways, let's just move on. So the proof P is
a type, and if we kind of have such a thingy, then we know, hey, we actually
proved it, okay? And in this way, we can write some various rules of logical
deduction, we know, which one of the most prominent being modus ponens. And
modus ponens is very elementary, and it just says, well, if you have a proof
of position P, and a proof of the implication P implies Q, well, then you
can construct a proof of Q. Hopefully not too surprising. Hopefully not too
surprising. And this is, as I said, rather elementary, but we can write this
indeed as the following. This is the rule, and if we just postulate that this
proof exists, well, then this is something we can use in the proof. And this
is basically what an axiom is, you just say this constant just exists, and you
can use it, okay? Now, this is all well and good, but this was just a first
approach, how one could build something like this. But you can instead now
do some simplifications, actually. First of all, this difference between the
proposition itself and the proof of the proposition, it's just annoying, really,
because, well, we basically just want to be at the same thing. We would love
for the proposition itself to be its proof, kind of. So not having this extra
proof keyword or anything. And how could one do this? Well, it's not that hard,
actually. We can have the following idea. We say that there's this type, we have
prop. Then we have instances of prop, p and q. But p and q themselves are, once
again, types. And we have terms. A term of p is the actual proof then, because
if you can basically instantiate this type, then you are able to show that this
proposition holds. The instantiation, the object of p, is the proof. And this
is now the breakthrough idea, or the $1 million idea, in a sense. And this is
now the beginning of this isomorphism, which was alluded to in the title of
this talk. The Curry-Howard isomorphism. It is all about now leveraging type
theory and the things we looked at when looking at the basics of Lean to really
do all of the work which is involved for doing the proving and the math. And
this actually works. Okay. So I want to iterate once again. So we have, what is
it? It's kind of, it may sound weird to you. Like, prop is something, then we
have p and q, which are of type prop. And these are, again, types. What are you
talking about? Let me try to clear this up. And it has, once again, to do with
this type hierarchy. And we can now fully, try to fully understand this type
hierarchy. Prop is actually the lowest, lowest, lowermost end of this hierarchy.
It is, prop is called, is a synonym for sort zero. Sort zero is the most basic
type universe. After sort zero comes sort one, known as type zero, also known as
just type. The normal type universe, you know, from other programming and so on.
And now we can do this little schematic here, this table here. And this table
will hopefully give you more insight than it will confuse you. Okay. So let's
step through it. At the top row specifies the type universe. There is prop,
which is the lowermost type universe. Then there is type, also known as type
zero. Then there is type one, type two, type three, and so on until forever.
And the next row now is a type, an example of a type that lives in this type
universe. For instance, in prop, there is a type which is known as true. True
is just always true, or P and Q. True is a great example because true always
holds, and there's a very basic introduction rule or a very basic constructor.
You can just have it. It's just a constant that exists. So you have your true,
and it then, if we go one to the right, look at the type universe type. What
does it contain? What is an example of a type in this type universe? Nat, or
nat, the natural numbers. Okay. Now we go one more to the right. Okay. Now
type one. What is an example of a type that is contained in type one? Well,
actually, type, because this is exactly what we said. In this hierarchy, each
new universe contains the previous type universe. The previous type universe as
an object, as a thing, as a type. Okay. So we have like this movement. So this
is, but it goes in this way. So these are not, it's not that only this thing
is contained in this type universe, but the previous one must be contained in
it. Okay. So hopefully this is not too confusing. And now we can look at the
lowermost row, which is the term. Okay. We give an example of a term of a type
that lives in this type universe. And for top, if we look at true, then there is
one very simple instance, which is just trivial. This is this constructor giving
you an instance, which you can get for free because true is trivially true.
And there is a trivial proof for the statement true. It's just true. There's
nothing to prove. Then on the, well, move on to the right. We have net. An
instance of this is five. Then we go one more. We have type universe one. Type
is a type contained in type one. Type universe one. You can go on, go on. Okay.
So hopefully this schematic was kind of helpful. Hopefully not too confusing.
Okay. Now, moving on, we have now seen the full hierarchy and now we're going
to look a little bit on how prop works. For instance, this change now of using
the instances of P as a proof for P will now allow for further simplifications.
And now we see the real first example of this isomorphism between logic and a
theory. And it is given by the fact that the implication error and the function
error are not only syntactically the same, but they are semantically the same.
If we look, if we are working in prop and we have a function from prop to prop,
this is the same thing as an implication. Because what it really means is that
if you have, are given an object of type prop, which is of type P, which is a
proof of P, and you have this function, well, then you have a way of getting an
object of type Q, meaning a proof of Q. So this is really an implication. If you
have a proof of P, you can deduce a proof of Q. This is the same thing. So this
implies construct, this syntax, not necessary, doesn't exist in Lean. It's just
a normal function error. And this is the first example of this isomorphism. Now
elaborating a little bit more, this isomorphism is very broad. For instance,
and this is how isomorphism is usually summarized as two little statements.
Propositions as types and proofs as programs. I think programs is kind of weird.
It should rather say instances or terms to be type theoretically correct. But
really it is what it's called probably because it involves kind of a computation
because the actual verification of the proof is done by doing type checking.
And this is basically the computation you don't have to. So it is called proof
as programs, right? Because this is the program you kind of type check. Oh
yeah, this is rather elementary. Implications are functions. And yeah, you can
go further. Now we can talk about some other stuff. We can, for instance, look
at how we now write some axioms which are just constants here. And use this.
And we can also look at that false is what the false exists. We should look
at how true is in a sense the easiest proposition to construct. But the least
powerful that exists. There is one introduction rule that doesn't need anything
to introduce true. But true doesn't allow you to do anything with it except that
you have a true. False is in a sense the hardest type to construct. There are
in a sense the hardest one to construct. But once you start one, you can prove
anything. And false is actually something that is used a lot. It's saying that
it's the hardest one to construct. It may be a weird thing to say. It's just in
some sense it is the hardest one. But you actually construct it often. And it's
a usual construct. Because in order to understand why, we need to understand
what negation is and what not p is. Not p is really just syntactic sugar for
something else. And this thing is p implies false. So if you have p, you get
a contradiction. Which makes sense. You would assume so that not p and not p
can be the case at the same time. So if you have a p, you get a contradiction.
It's sensible. How can one use this? Well, there are two things you need to
figure out. How do I introduce a proof of not p? And how do I use a proof of
not p? Introducing it is not too hard. And do this. And you do this. And yes,
this is the power of it. Okay, now let's talk a little bit about constructive
mathematics. Constructive mathematics is an interesting branch which is very
computational. And everything we've done so far in Lean is actually constructive
mathematics. And what does this really mean? Well, this means that every time
we have a proof of something, we actually really have constructed a way of
getting there. This might sound a little bit abstract. But you might have
heard about, you might have come across, you probably came across in your
studies, nonexistence proofs which are not constructive. What does this mean?
This means that you somehow prove that a mathematical object exists, certify
fulfilling some properties. But you don't actually have an example of it. This
is a non-constructive existence proof then. Because you know it exists, but you
don't know what it is, where it is. There's no recipe for constructing it just
because you have to prove that it exists. Such a proof is non-constructive.
And this is with the things I've shown you so far in Lean not possible. And
there is a discussion to be had around this. In a sense, only allowing for
constructive proof is powerful. It's powerful. Why? Well, you can be sure that
for any theorem that you have some way of exhibiting the object that fulfills
this theorem. You always have a recipe for constructing. And this is because
you always have instances of proofs. And you just use function applications.
Then round and gather yada. I also need to talk about proof irrelevance. This
is something that probably makes props special. Blah, blah, blah. But in another
sense, it's very restrictive being only constructive. Because you lose some
assumptions. You lose some things you can do. A lot of theorems in math have
only been proven in a non-constructive way. Because in a constructive universe,
maybe there wouldn't even be a proof. And actually, and I think this is now very
interesting. Because you can basically... The non-constructiveness of a lot of
math we do, this follows from one very, very, very basic axiom. Which is the
law of the excluded middle. And it just states the following. For any proper
position p, we know p or not. And this really contains a lot. This theorem you
cannot construct in Lean without just assuming it has an axiom. Because it is
non-constructive. Why? Because you just say you know that p or not p. But you
don't know which. You don't know which it is. You just say it's either of them.
So you don't know in which instance you are. And you lose the constructiveness.
Because you don't know what you're talking about. Which object you're talking
about anymore. And this then allows double negation. You wouldn't have double
negation otherwise. And you can do a lot of math without double negation. And
it's really just not there. And this even means that really in a sense, without
the law of the excluded middle, and just having constructive mathematics, logic
doesn't need to be binary. So nothing tells you that there's only true and
false. And then you think no, this is how it is. It doesn't need to be binary
necessarily. But this is more a philosophical debate I don't want to get into
right now. We can talk afterwards if you want to. I still have some questions
in this regard. I don't understand this either all the way. But I think it's
really interesting. And so without these axioms, a lot of things are missing.
Also, a proof by contradiction is non-constructive. Without this axiom, you
cannot do any proof by contradiction. Because a proof by contradiction really is
kind of weird. Because you just say, I assume that something is the case. Then
I show that it leads to a contradiction. And so I get a false. And from false
follows anything. But really, you haven't provided recipes to then construct
anything. So it's non-constructive. I think these are very interesting things.
So yeah. Anyways. After this diversion of constructive or non-constructive
mathematics, I want to now do some cool stuff in me. And do some more hands-on
stuff. Do some actual fear improvement. And for this, we need to still do look
at a small amount of fear. I just need to introduce a small theoretical new
thing. Which is... Do anything meaningful. I still want to introduce predicate
logic. For predicate logic... What changes for predicate logic? Predicate logic
is a more general time proposition. Because it now is talking about predicates.
Which are functions that take some element. Take an argument. And decide if
the proposition is true or not. If it is true for this object. Okay. Is it
true? For instance, one could define the following predicate. Is even takes a
natural number. And then we can have... See if the case is not depending on...
Interesting. And we then can also introduce the for all quantifier. Which is
really just... Which is also... Why? Well, this is really the same thing. You
just give in. And then you will get the proof out. That this is the case for
these two. And even more. You can interp... Which is written as not for all.
Which you hopefully still know from a logic course. That this is actually the
same thing. And so this is actually this. And this is actually this. So you see
how we now build up more and more complex types. That basically are just basic
function applications in some type universe. But now we do some actual... We
keep on doing more mathy stuff. And it boils down to very simple type checking
tasks. Okay. Having introduced... Now I want to... There's even an interesting
interpretation for what for all and the existence quantifier are. Well, you
can think of the for all quantifier as the biggest thing in the universe.
And then you can also think of the for all quantifier as the biggest thing in
the universe. The biggest conjunction you can have. You just specify a type
universe. And then all of these types are inside all of the instances. All of
the terms are inside of this conjunction. So it's a conjunction. An infinitely
big conjunction of all the things. And or what is existence? Well, existence
is a disjunction. The biggest disjunction you can have. You have all the terms
of this type. And you just know, well, it's true for one of them. In this big
or, there is one element which makes the whole expression true. I think that
interesting point. I think that's an interesting point. Okay. Now we've seen...
Okay. Guys, enough theoretical stuff. Your head is probably already steaming.
Not sure. I might make you steam even more now. But depending on how, if you
prefer doing some actual theorem proving in Lean using by writing some code. And
we'll try to do some. And I've decided that I was thinking about either doing
some group theory. Or some things with relations such as equivalence relations
or partial order relations. And I must say, and to keep it more approachable, I
decided to do relations. Just do some relations. Okay. What is a relation? You
might remember relation from discrete mathematics. And Terry was used for a set
theoretic perspective. But really, type theory is here more fundamental than set
theory. And relations are also something you can define at a more fundamental
level without referring to sets. And basically, it just, the question is always,
is if we have two elements A and B. These are type universes. It's a different
meaning. Something else. That's a different approach. Of doing the foundations
of mathematics. And instead, Alec talked of Stolz. Okay. And so for a relation,
this is just a function. Written as prop, arrow, prop, arrow, prop. And it
takes two. No, it's alpha to alpha to prop. And it just takes two elements of
some type. And looks and tells, are they in relation or not? Okay. And let's do
some basic equivalence relation stuff. In order to define, we just first need
to define an equivalence relation. Okay. So we will do this in Lean. And also
look at some more familiar math notation. So we have equivalence relation tilde,
which is exactly then an equivalence relation. If it is reflexive, meaning for
all arguments. And the argument is in relation with itself. Then it must be
symmetric, meaning if two arguments A and B are in relation, then B must also
be in relation with A. And then we have transitivity, meaning if we have A is
in relation with B. And B is in relation with C. Then A must also be in relation
with C. And so in Lean, we can write it as this. Now we coincide. Okay, guys.
Time's actually up. But I guess time's up and I'm, I don't know. I'm sorry.
This, my talk was very, very long. I'm sorry. I'm sorry for my, my talk was
very, very long. I hope it was more or less interesting to you. I hope you found
some of the topics here. At least a tiny bit. If you just get a fraction of how
excited I'm about this stuff, then that is already, probably already blasted
away. Any questions, please? I don't know what I was talking about. Maybe you
probably have a lot of questions. So, hey, it's just, these topics are very,
I think these are very, very difficult topics. And I still haven't wrapped my
head around it myself. But I, I've been learning a lot in the last few days. And
hey, if you've got any questions, ask me. I'll think, I'll consider if I can say
anything about it. And if I can, I will. And otherwise, no problem. I'd love to
talk to you. I would love to talk to you guys. Bye-bye.

